\documentclass{beamer}
%\usepackage[margin=3.5cm]{geometry}
%\usepackage[latin1]{inputenc}
%\usepackage[T1]{fontenc}
%\usepackage[english]{babel}
%\usepackage{graphicx}
%\usepackage{amsmath}
%\usepackage{amsthm}
%\usepackage{mathtools}
%\usepackage{listings}
%\usepackage{qtree}
%\newtheorem{thm}{Theorem}
%\theoremstyle{definition}
%\newtheorem{defi}{Definition}
%\newtheorem{nota}{Notation}
%\theoremstyle{remark}
%\newtheorem{rem}{Remark}
%\theoremstyle{proposition}
%\newtheorem{prop}{Proposition}
%\newtheorem{lem}{Lemma}
\setbeamertemplate{caption}[numbered]
\usepackage{amsmath}
\usepackage[nodayofweek,level]{datetime}

\begin{document}
	\begin{frame}
		\title{Communities and crime}
		\subtitle{Prediction of violent crime in the USA}
		\author{Marie \textsc{Lontsie} Yanis \textsc{Bosch}}
		\maketitle
	\end{frame}

	\begin{frame}{Outline}
		\begin{enumerate}
                        \item Communities and Crime
			\item Dataset
			\item Preprocessing
			\item Regression
			\item Correlation Analysis
			\item Performance analysis
			\item Conclusion
		\end{enumerate}
	\end{frame}

	\begin{frame}{The dataset}
		\begin{itemize}
			\item Data sources:
				\begin{itemize}
					\item Socio-economic data from the $1990$ US Census
					\item Law enforcement data from the $1990$ US LEMAS survey
					\item Crime data from the $1995$ FBI UCR
				\end{itemize}
			\item Creator: Michael Redmond, La Salle University, Philadelphia
			\item Date: \formatdate{13}{7}{2009}
		\end{itemize}
	\end{frame}

	\begin{frame}{The dataset}
		\begin{itemize}
			\item Size: $1994$ rows, $128$ columns
			\item Example attributes: police officers per $100$K population, median rent,...
			\item Goal: Prediction of violent crime in the USA
		\end{itemize}		
	\end{frame}
	\begin{frame}{The dataset}
		\begin{itemize}
			\item As in most countries, violent crime is driven by socio-economic factors
			\item There seems to be a strong link between income inequality and crime
			\item Does our data confirm this?
			\item Which of these factors are of the highest importance?
		\end{itemize}
	\end{frame}
	\begin{frame}{Preprocessing}
		\begin{itemize}
			\item Before studying these correlations we must make sure our data is clean
			\item The values are already normalised, we must thus turn our attention to missing values
		\end{itemize}
	\end{frame}
	\begin{frame}{Preprocessing}
		\begin{center}
			\begin{table}
				\resizebox{10cm}{!}{
					\begin{tabular}{| c | c || c | c |} 
						\hline
	 					Column Name & Missing values & Column Name & Missing values\\ \hline
						PolicReqPerOffic & $1675 (84\%)$ & PolicAveOTWorked & $1675 (84\%)$ \\ \hline
						PolicPerPop & $1675 (84\%)$ & RacialMatchCommPol & $1675 (84\%)$ \\ \hline
						PctPolicWhite & $1675 (84\%)$ & PctPolicBlack & $1675 (84\%)$ \\ \hline
						PctPolicHisp & $1675 (84\%)$ & PctPolicAsian & $1675 (84\%)$ \\ \hline
						PctPolicMinor & $1675 (84\%)$ & OfficAssgnDrugUnits & $1675 (84\%)$ \\ \hline
						NumKindsDrugsSeiz & $1675 (84\%)$ & LemasSwFTFieldPerPop & $1675 (84\%)$ \\ \hline
						LemasTotReqPerPop & $1675 (84\%)$ & LemasSwFTFieldOps & $1675 (84\%)$ \\ \hline
						LemasSwFTPerPop & $1675 (84\%)$ & PolicCars & $1675 (84\%)$ \\ \hline
						PolicOperBudg & $1675 (84\%)$ & LemasPctPolicOnPatr & $1675 (84\%)$ \\ \hline
						LemasGangUnitDeploy & $1675 (84\%)$ & LemasSwornFT & $1675 (84\%)$ \\ \hline
						PolicBudgPerPop & $1675 (84\%)$ & LemasTotalReq & $1675 (84\%)$ \\ \hline
						OtherPerCap & $1 (0.05\%)$ & & \\ \hline
					\end{tabular}}
				\caption{Total number of rows: $1994$}
			\end{table}
		\end{center}
	\end{frame}

	\begin{frame}{Preprocessing}
		Listwise deletion:
		\begin{itemize}
			\item = Method for handling missing data
			\item Delete columns or rows that have any missing data at all
			\item Very simple method to deal with missing data
			\item Loss of information, and thus loss in the quality of the prediction
			\item Good method so long as we retain sufficient power after deletion
		\end{itemize}
	\end{frame}

	\begin{frame}{Preprocessing}
		Imputation:
		\begin{itemize}
			\item = Method for handling missing data
			\item Replace missing values with substituted data
			\item Ex: Median, Average,...
			\item Less loss of information
			\item May introduce bias in the correlation
			\item Leads to lower standard errors, which may lead to Type 1 errors
		\end{itemize}
	\end{frame}

	\begin{frame}{Preprocessing}
		Why can we use listwise deletion on the columns with $84\%$ of missing data?
		\begin{itemize}
			\item Most of the entries are missing, thus we don't lose too much data
			\item We have very little data left to base our imputation on, which would make it a bad choice
		\end{itemize}
	\end{frame}

	\begin{frame}{Preprocessing}
		How do we handle the one missing entry in the OtherPerCap column?
		\begin{itemize}
			\item Delete the column, but we would lose $1994$ entries
			\item Use imputation, which should work well in this case
			\item Delete the row, and lose one out of $1994$ rows = minimal loss of information
		\end{itemize}
		We deleted the row containing the missing value to keep our code as simple as we can
	\end{frame}
	\begin{frame}{Regression}
              
        \end{frame}
	\begin{frame}{Correlation analysis}
		\begin{itemize}
			\item Before applying a regression algorithm, it would be interesting to check which predictors are significant
			\item Thus we plot a graph with the correlation between the predictors and violent crime
			\item We exclude all predictors with a correlation that lies close to $0$
		\end{itemize}
	\end{frame}
	\begin{frame}{Correlation analysis}
		\begin{figure}[h]
			\includegraphics[width=10cm]{correlation.png}
			\centering
		\end{figure}
	\end{frame}
	\begin{frame}{Correlation analysis}
		INSERT CLOSER ANALYSIS OF SOME OF PREDICTORS WITH BEST CORRELATION (OR INVERSE CORRELATION)
	\end{frame}
	\begin{frame}{Regression}
		\begin{itemize}
			\item Given that our response variable is continuous, we have to perform regression to predict it
			\item Idea: Use random forest regression
		\end{itemize}
	\end{frame}
	\begin{frame}{Regression}
		What is random forest regression?
		\begin{itemize}
			\item Based on ensemble learning
			\begin{itemize}
				\item = method where multiple ML algorithms are combined
			\end{itemize}
			\item Utilises subsets of the data to create multiple trees (= bagging)
			\item The obtained results are averaged to create the final result
		\end{itemize}
	\end{frame}
	\begin{frame}{Regression}
		What are the advantages of random forest regression?
		\begin{itemize}
			\item Performs well with little to no hyperparameter tuning
			\item Rarely overfits
			\item Low sensitivity to noise
			\item Good at noticing general patterns in the data
		\end{itemize}
	\end{frame}
	\begin{frame}{Regression}
		What are the disadvantages of random forest regression?
		\begin{itemize}
			\item Bad at extrapolation
			\item Makes predictions only in the range of data contained in the training set
		\end{itemize}
	\end{frame}
	\begin{frame}{Regression}
		Why can we use random forest regression?
		\begin{itemize}
			\item Our data seems to be diverse enough to cover a realistic range of crime rates
			\item It seems unlikely that we might have to predict a crime rate that is much higher than in our training set
			\item We have quite a few predictors left, even after cleaning, thus overfitting could be an issue
		\end{itemize}
	\end{frame}
	
	\begin{frame}{Sources}
		\begin{itemize}
			\item https://en.wikipedia.org/wiki/Listwise\_deletion
			\item https://en.wikipedia.org/wiki/Imputation\_(statistics)
			\item https://www.theanalysisfactor.com/mean-imputation/
			\item https://www.theanalysisfactor.com/when-listwise-deletion-works/
			\item https://cnvrg.io/random-forest-regression/
			\item https://minds.wisconsin.edu/bitstream/handle/1793/77496/Violent\%20Crime.pdf?sequence=1
		\end{itemize}
	\end{frame}
\end{document}








